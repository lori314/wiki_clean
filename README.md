## 任务一：维基百科数据清洗

### 1. 任务目标

对中文维基百科的数据dump进行下载、解析、清洗和格式转换，最终输出为适合大语言模型预训练的、干净的、分块存储的JSON Lines (`.jsonl`)格式文件。

### 2. 项目结构

```
task1_wiki_cleaning/
├── 01_clean_wiki_to_jsonl.py    # 主清洗脚本
├── data/                          # (建议) 存放原始数据dump
│   └── zhwiki-....xml.bz2
├── cleaned_data_samples/          # 存放1000条清洗后的样例数据
│   └── samples.jsonl
└── output_dir/                   # 存放完整清洗后的数据(运行python脚本会自动创建改目录）
    ├── outputfile0.jsonl
    └── outputfile1.jsonl
    ...
```

### 3. 如何运行

请严格按照以下步骤，以保证结果的可复现性。

**a. 环境搭建 (Environment Setup)**

```bash
# 1. 克隆本项目仓库
git clone <你的仓库SSH或HTTPS链接>
cd <仓库名>

# 2. 创建并激活Python 3.10虚拟环境 (以Linux/WSL为例)
python3.10 -m venv .venv
source .venv/bin/activate

# 3. 安装所有依赖
pip install -r requirements.txt
```

**b. 阶段一：使用`wikiextractor`进行粗加工**

此步骤将原始的维基百科dump文件，解析并提取为结构化的JSON Lines文件。

```bash
# 1. 准备数据
#    将下载的维基百科dump文件 (例如 `zhwiki-....xml.bz2`) 
#    创建并放置在 `data/` 目录下。

# 2. 执行粗加工提取
#    运行wikiextractor。此命令会自动利用所有CPU核心进行并行处理。
#    处理完整数据可能需要数小时。建议先用 --bytes 100M 参数进行测试。
wikiextractor --json -o extracted_test data/zhwiki-....xml.bz2
```
*   **输入:** `data/zhwiki-....xml.bz2`
*   **输出:** 脚本执行完毕后，会在`wiki_clean/`下创建一个名为`extracted_test/`的文件夹，里面包含了经过初步提取的、分片的`wiki_*`文件。

**c. 阶段二：执行自定义脚本进行精加工**

此步骤会读取上一阶段的产出，进行更深度的清洗、转换和分块存储。

```bash
# (请确保你仍然在 wiki_clean 目录下)

# 运行精加工脚本
python 01_clean_wiki_to_jsonl.py
```
*   **输入:** `extracted_test/` 文件夹中的所有内容。
*   **输出:** 清洗后的最终数据块，将保存在`output_dir/`目录下。此后，运行`create_samples.py`，在`cleaned_data_samples/`目录下会生成一份1000行的样例数据。（题目要求的1000个样例即保存在该目录下）

### 4. 清洗策略与实现细节

我的解决方案包含两个主要阶段：

**阶段一：使用`wikiextractor`进行粗加工**
- **工具选型:** 面对超过12G的原始XML数据，我没有选择自己编写复杂的XML解析器。通过调研，我选择了业界标准的`wikiextractor`工具，因为它能高效地处理XML结构、自动跳过非正文页面（如讨论页、重定向页），并完成大部分基础的Wiki标记清理。
- **执行环境:** 为了解决`wikiextractor`的多进程`fork`模型与Windows不兼容的问题，以及后续大规模数据处理的性能需求，我最终决定将此计算密集型任务部署在**WSL(Ubuntu)**环境中执行。

**阶段二：使用自定义Python脚本进行精加工**
在`wikiextractor`处理后，我编写了`01_clean_wiki_to_jsonl.py`脚本来执行更精细的清洗，主要包括：
1.  从wiki_文件中的json检查并提取text字段，并把其title,id,url整理成一个字典存入meta
2.  由于对json字符串逐行处理，可以在遍历写入前使用一些策略来更具具体需要筛选，比如繁体转化为简体，过滤掉过短的数据之类，此脚本对过短数据进行了筛选，其它功能可以很方便地在需要时扩展

### 5. 遇到的挑战与解决方案

在完成这个任务的过程中，我遇到并解决了一系列真实世界开发中常见的环境配置和兼容性问题。这个过程比编写代码本身更具挑战性，也更有收获。

-   **挑战1：大规模文件的探查。**
    -   **问题:** 无法用常规编辑器打开12G的原始文件。
    -   **解决方案:** 学习并采用了`bzcat | less`的流式处理命令，在不消耗内存的情况下，成功探查了文件结构。
-   **挑战2：Python版本与库的兼容性。**
    -   **问题:** 在Python 3.11/3.12下，`wikiextractor`因正则表达式语法不兼容而报错。
    *   **解决方案:** 认识到多版本管理的重要性，学习并实践了为项目创建指定版本（Python 3.10）的虚拟环境，从根本上解决了问题。
-   **挑战3：跨平台开发环境的差异。**
    *   **问题:** 遇到了Windows的PowerShell执行策略、与Linux的`fork`进程模型不兼容、以及新版Ubuntu的`externally-managed-environment`等一系列平台相关问题。
    *   **解决方案:** 这一系列的挑战，让我对虚拟环境、PPA软件源、以及不同操作系统的底层差异有了更深刻的实践理解。最终我选择WSL作为主力开发环境，因为它更贴近生产环境。

### 6. 清洗结果样例

清洗后的数据严格遵循`jsonl`格式，每行一个JSON对象，包含`text`和`meta`两个字段。

```json
{
    "text": "数学是研究数量、结构以及空间等概念及其变化的一门学科，屬於形式科學的一種。數學利用抽象化和邏輯推理，從計數、計算、量度、對物體形狀及運動的觀察中產生。數學家們拓展這些概念，為了公式化新的猜想以及從選定的公理及定義中建立嚴謹推導出的定理……",
    "meta": {
        "title": "数学",
        "id": "13"
    }
}
{
    "text": "哲學是研究普遍的、基本問題的學科，包括存在、知識、價值、理性、心靈、語言、人生、道德等領域。哲學與其他學科不同之處在於哲學有獨特之思考方式，例如批判性的思考、系統化的方法以及理性論證……",
    "meta": {
        "title": "哲學",
        "id": "18"
    }
}
```
*（请注意：为了文档的简洁，这里的样例是经过格式化的。实际的`.jsonl`文件中，每个JSON对象都在一行内。）*

---
